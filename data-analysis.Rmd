---
title: "Traffic Data Analysis"
output: 
    html_document:
      toc: true
      toc_float: false
      collapsed: false
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
library(knitr)
library(stargazer)
library(MASS)
library(pracma)
library(tidyr)
library(lubridate)
library(XML)
library(pastecs)
```

### Reading and Processing Data

This section is responsible for reading the data and cleaning it.

#### Data Reading

```{r, tidy=TRUE, cache=TRUE}
be2olak.data <- read.csv("all-semi-unique.csv")
names(be2olak.data)
str(be2olak.data)
dim(be2olak.data)
```

***

#### Data Cleaning

The aim of this section:

* Remove not needed columns
* Remove duplicated rows
* Remove NAs

#### Removing not needed columns

By analyzing the distribution of some columns, I found that some of the columns only have one value, so these columns are not needed and should be deleted from the data.

```{r, tidy=TRUE, cache=TRUE}
column_count = sapply(be2olak.data, function(x) length(unique(x)))
be2olak.data <- be2olak.data[, !names(be2olak.data) %in% names(column_count[column_count==1])]
```

Moreover, the 6 columns (rd.stid, rd.hr, rd.mn, rd.rp.fullnm, rd.img, rd.rp.img) are meaningless and want add anything to the data so I will remove them:

```{r, tidy=TRUE, cache=TRUE}
columns.delete = c("rd.stid", "rd.hr", "rd.mn", "rd.rp.fullnm", "rd.img", "rd.rp.img")
```

```{r, tidy=TRUE, cache=TRUE}
be2olak.data <- be2olak.data[, !names(be2olak.data) %in% columns.delete]
```

```{r, tidy=TRUE, cache=TRUE}
stargazer(be2olak.data, type = "text", title = "Descriptive statistics")
```

***

#### Removing Duplicated Rows

Now we need only to keep the unique rows and remove any duplicates. In order to do that, I checked the number of unique rows with respect to more than one combination of columns to get what really affect the data.

```{r, tidy=TRUE, cache=TRUE}
# with respect to comment id
nrow(unique(be2olak.data[c("rd.rp.cmid")]))
# with respect to status id, and comment id
nrow(unique(be2olak.data[c("rd.rp.stid", "rd.rp.cmid")]))
# with respect to road index, status id, and comment id
nrow(unique(be2olak.data[c("rd.ri", "rd.rp.stid", "rd.rp.cmid")]))
```

From the results presented above, the combination of (road index, status id, and comment id) would make the report entry unique.

So now I will remove duplicated rows.

```{r, tidy=TRUE, cache=TRUE}
be2olak.data <- be2olak.data[!duplicated(be2olak.data[,which( colnames(be2olak.data) %in% c("rd.rp.cmid", "rd.rp.stid", "rd.ri") )]),]
```

Checking data summary again after removing the duplicated rows:

```{r, tidy=TRUE, cache=TRUE}
dim(be2olak.data)
stargazer(be2olak.data, type = "text", title = "Bey2ollak Data")
summary(be2olak.data)
str(be2olak.data)
```


***

#### Investigate NAs

First, lets check the proportion of NAs:
```{r, tidy=TRUE, cache=TRUE}
length(be2olak.data[is.na(be2olak.data)])/(ncol(be2olak.data)*nrow(be2olak.data)) 
```

And from the summary results the Nas are there because of 2 columns (rd.rp.stid , rd.rp.rpImg)

So for rd.rp.rpImg, it means that the reporter has uploaded a photo for the road. Otherwise, it means the he/she did not.

So for my analysis, I am not concerned with photo itself, I am only concerned with the uploading action. So I will remove this column after adding one that has "YES" or "NO" representing whether this report has an uploaded photo or not.

```{r, tidy=TRUE, cache=TRUE}
be2olak.data$rp.has.photo <- as.factor(ifelse(is.na(be2olak.data$rd.rp.rpImg), "NO", "YES"))
be2olak.data$rd.rp.rpImg <- NULL
```

So check on the proportion of NAs again

```{r, tidy=TRUE, cache=TRUE}
length(be2olak.data[is.na(be2olak.data)])/(ncol(be2olak.data)*nrow(be2olak.data)) 
```


```{r, tidy=TRUE, cache=TRUE}
dim(be2olak.data)
stargazer(be2olak.data, type = "text", title = "Bey2ollak Data")
summary(be2olak.data)
str(be2olak.data)
```

Now lets get to the other column which is the rd.rp.stid

#### Exploring the NAs of the rd.rp.stid column

Note, here is a function taken from online resource that I will be using in my analysis

```{r, tidy=TRUE, cache=TRUE}
freqfunc <- function(x, n){
  tail(sort(table(unlist(strsplit(as.character(x), ", ")))), n)
}
```

Ok, Let get only the rows where rd.rp.stid is NA

```{r, tidy=TRUE, cache=TRUE}
missing.stid <- be2olak.data[is.na(be2olak.data$rd.rp.stid), c("rd.rp.cm", "rd.nm")]
str(missing.stid)
summary(missing.stid)
ggplot(missing.stid, aes(x = rd.nm, fill = rd.nm)) + geom_bar()
```

checking most of the comments in those reports

```{r, tidy=TRUE, cache=TRUE}
freqfunc(missing.stid$rd.rp.cm, 60)
```


So after analyzing the rows that has stid with NAs so mainly they are highway roads and roads connecting between cities.

Moreover, The comments are mainly about asking about radar and reporting radars or clear statuses.

Those type of roads would need another analysis, but for my analysis I would remove those entries from the original data frame since they are not relevant.

```{r, tidy=TRUE, cache=TRUE}
be2olak.data <- be2olak.data %>% filter(!is.na(rd.rp.stid))
```

Now check that all NAs have been removed

```{r, tidy=TRUE, cache=TRUE}
length(be2olak.data[is.na(be2olak.data)]) 
```

***

### Feature Engineering

In this section, I will do some feature engineering.

#### Format Crawl Date in a better way

The crawling date is presented in a way that we could make a use of to get the other columns that might help in viewing relations between columns.

Moreover, it will allow us to get the actual date of report from a user, which in case of "za7ma" reports for example, it will get the actual time of the phenomenon.

So to do this, there are 2 main steps:

* First, is to split the crawl date into week day, month, month day, crawl-date (as hours - minutes - seconds) only.

* Then form a new column representing this a formated crawling date.

* Then get the actual reports date (taking into consideration the difference between EET and UCT).

```{r, tidy=T}
be2olak.data <- be2olak.data %>% separate(crawl_date, c("week.day", "month", "month.day", "crawl.date"), extra = "drop", sep = "[ ]+", convert = T, remove = T) %>% unite(crawl.time, week.day, month, month.day, crawl.date, sep = " ", remove = F)
be2olak.data$formated.date <- as.POSIXct(strptime(be2olak.data$crawl.time, format="%a %b %d %H:%M:%S"))
be2olak.data$week.day <- as.factor(be2olak.data$week.day)
```

```{r, tidy=TRUE}
#be2olak.data <- be2olak.data %>% separate(crawl_date, c("week.day", "month", "month.day", "crawl.date"), extra = "drop", sep = "[ ]+", convert = T, remove = T) %>% unite(crawl.time, week.day, month, month.day, crawl.date, sep = " ", remove = F)

#be2olak.data$formated.date <- as.POSIXct(strptime(be2olak.data$crawl.time, format="%a %b %d %H:%M:%S"))
#be2olak.data$week.day <- as.factor(be2olak.data$week.day)
```

***

#### Get Comment's Actual Time

```{r, tidy=TRUE}

# Adding 2 hours to get Egypt's local time instead of UTC
# Remove the duration of the report has been posted presented by rd.rp.hr and rd.rp.mn

be2olak.data$comment.time <- be2olak.data$formated.date + hours(2) - hours(be2olak.data$rd.rp.hr) - minutes(be2olak.data$rd.rp.mn)
```

Then remove not needed columns in this case

```{r, tidy=TRUE}
be2olak.data$formated.date <- NULL
be2olak.data$rd.rp.hr <- NULL
be2olak.data$rd.rp.mn <- NULL
be2olak.data$crawl.date <- NULL
be2olak.data$crawl.time <- NULL
# Moreover, month would not be needed since it is the same for all the data
be2olak.data$month <- NULL
```

Then check the stats of the dataframe

```{r, tidy=TRUE}
str(be2olak.data)
summary(be2olak.data)
glimpse(be2olak.data)
```

***

#### Add the city for each entry

Another thing that could be done related to augmenting the data, is to add a column representing the city of each the road being reported on, whether it is "cairo" or "alex". And this is done in the next section.

First, get the ids of the roads in Cairo.

```{r, tidy=TRUE}

# Get Cairo Road ids

doc <- htmlParse("http://www.bey2ollak.com/Bey2ollak/Traffic?action=getTraffic&ver=1.0&w=320&h=240&deviceType=10&lang=1&protocol=1&city=0&lang=1")
cairo.roads.id <- sapply(getNodeSet(doc, "//ri"), function(x) as.integer(xmlValue(x)))
length(cairo.roads.id)
```

Then, get the ids of the roads in Alex.

```{r, tidy=TRUE}

# Get Alex Road ids

doc <- htmlParse("http://www.bey2ollak.com/Bey2ollak/Traffic?action=getTraffic&ver=1.0&w=320&h=240&deviceType=10&lang=1&protocol=1&city=1&lang=1")
alex.roads.id <- sapply(getNodeSet(doc, "//ri"), function(x) as.integer(xmlValue(x)))
length(alex.roads.id)
```

Then, add column representing the city in each row in the dataframe.

```{r, tidy=TRUE}
be2olak.data$city[(be2olak.data$rd.ri %in% cairo.roads.id)] = "cairo"
be2olak.data$city[(be2olak.data$rd.ri %in% alex.roads.id)] = "alex"
#be2olak.data$city <- as.factor(be2olak.data$city)
ggplot(be2olak.data) + geom_bar(aes(x = city), fill = "gray")
```

Now check in who many rows city is NA.

```{r, tidy=TRUE}
unique(be2olak.data$rd.nm[is.na(be2olak.data$city)])
```

So this made me suspicious why this happened. So I will discuss the findings at the end of this EDA. But for now I have two options either to add the missing cities since they are only 5 roads. Or to remove that city column since my findings was not promising.

So I will take the second choice.

```{r, tidy=TRUE}
be2olak.data$city <- NULL
```

#### Get the main road name from rd.nm

Another thing that could be done is to make a new column (road.main) from the rd.nm which is the first part of the road name, since this might introduce some visualisations or relations between two sides of the road.

```{r, tidy=TRUE}
be2olak.data <- be2olak.data %>% separate(rd.nm, c("road.main"), extra = "drop", sep = ";", convert = F, remove = F)
be2olak.data$road.main <- as.factor(be2olak.data$road.main)
```

***

### Plotting graphs for the different columns

First, Plotting the integer columns and Factored columns.

```{r, tidy=TRUE}
# Integer Columns
ggplot(be2olak.data, aes(x=rd.rp.stid)) + geom_bar()
ggplot(be2olak.data, aes(x=rd.ri)) + geom_bar()
ggplot(be2olak.data, aes(x=month.day)) + geom_bar()
ggplot(be2olak.data, aes(x=rd.new)) + geom_bar()
ggplot(be2olak.data, aes(x=rd.strq)) + geom_bar()
ggplot(be2olak.data, aes(x=rd.cmrq)) + geom_bar()

# Factored Columns
ggplot(be2olak.data, aes(x=rd.nm)) + geom_bar()
ggplot(be2olak.data, aes(x=road.main)) + geom_bar()
ggplot(be2olak.data, aes(x=week.day)) + geom_bar()
ggplot(be2olak.data, aes(x=rp.has.photo)) + geom_bar()
```

Second Plotting the time column
```{r, tidy=TRUE}
ggplot(be2olak.data, aes(x=comment.time)) + geom_density()
```


### Visualising and Stating statistics for part of data

Of course one of the most important questions, that I am interested to prove is that that



### Plotting graphs for the different columns

```{r, tidy=TRUE, cache=TRUE}
status <- be2olak.data$rd.rp.stid
status.freq <- table(status)
#pie(status.freq, radius = 1)
print(status.freq)
str(status.freq)
barplot(status.freq)
```

```{r, tidy=TRUE, cache=TRUE}
hist(status,xlab = "Comment status",10)
```

```{r, tidy=TRUE, cache=TRUE}
stid.nas <- sum(is.na(be2olak.data$rd.rp.stid))
print(stid.nas)
stid.nas/(nrow(be2olak.data)) 
ggplot(be2olak.data) + geom_bar(aes(x=rd.rp.stid), fill="gray")
```

By Montoring Be2ollak website, I was able to find most of those status look like:

* 1 --> :D
* 2 --> :)
* 3 --> :|
* 4 --> :(
* 5 --> :'(
* 6 --> ?
* 7 --> Danger Skull
* 10 --> lamb

However, 8 and 9 were difficult to find in the feed.


```{r, tidy=TRUE, cache=TRUE}
#write.csv(be2olak.data, file = "MyData.csv",row.names=FALSE)
```

***

```{r, tidy=TRUE, cache=TRUE}
#ri.freq <- table(missing.stid$rd.ri)
#print(ri.freq)
#str(ri.freq)
#barplot(ri.freq, main = "Frequency of roads missing stid")
#func <- missing.stid$rd.ri*100.0/nrow(missing.stid)
#plot(x = missing.stid$rd.ri, y = func)
```


```{r, tidy=TRUE, cache=TRUE}
#write.csv(missing.stid, file = "MissingDataStid.csv",row.names=FALSE)
```


```{r, tidy=TRUE, cache=TRUE}

```

***
### Hypotheses

Here are some ideas for Hypotheses that could be tried

* [1] The ratio of negative reports in Cairo is much higher than in Alex.
* [2] The ratio of negative reports on Firday is less than any other day in the week.
* [3] In (3otl, 7adsa, khatar), the probabitity of uploading an image in a report is significantly higher than in normal times.
* [4] Leaving the default message of a status is higher than of write customized ones.
* [5] [7:30 am - 9 am] and [2 pm - 4 pm] have higher rate of negative reports than in other times of the day.

***
***
***

CMRQ and STRQ

```{r, tidy=TRUE, cache=TRUE}
strq.cmrq <- be2olak.data[which(be2olak.data$rd.strq == be2olak.data$rd.cmrq),]
nrow(strq.cmrq)
head(strq.cmrq[,c(2:13)])
```

```{r, tidy=TRUE, cache=TRUE}
unique.cmid <- aggregate(rd.rp.stid ~ rd.rp.cmid, be2olak.data, length)
duplicated.cmid <- unique.cmid[ which(unique.cmid$rd.rp.stid > 1), ]
head(duplicated.cmid)
tail(duplicated.cmid)
```


```{r, tidy=TRUE, cache=TRUE}
duplicated.cmid.vector <- rbind(duplicated.cmid$rd.rp.cmid)
#print(duplicated.cmid.vector)
duplicated.rows <- be2olak.data[ which(be2olak.data$rd.rp.cmid %in% duplicated.cmid.vector), ]
nrow(duplicated.rows[!duplicated.rows$rd.rp.nm %in% c("bey2ollakgps"),])
```

From above we reached that duplicated rows are only existing when rd.rp.nm is "be2ollakgps"

```{r, tidy=TRUE, cache=TRUE}
stid10.percentage <- nrow(duplicated.rows[which(duplicated.rows$rd.rp.stid == 10),]) * 100.0/ nrow(duplicated.rows)
print(stid10.percentage)
```



***
***
***
```{r, tidy=TRUE, cache=TRUE}
missing.stid <- missing.stid %>% separate(rd.nm, c("road.main"), extra = "drop", sep = ";", convert = F, remove = F)
missing.stid$road.main <- as.factor(missing.stid$road.main)
#missing.stid$rd.ri <- as.factor(missing.stid$rd.ri)
summary(missing.stid)
#print.data.frame(missing.stid)
ggplot(missing.stid, aes(x=road.main, y=rd.nm)) + geom_jitter()
```



```{r, tidy=TRUE, cache=TRUE}

```



```{r, tidy=TRUE, cache=TRUE}
#ri.freq <- table(missing.stid$rd.ri)
#print(ri.freq)
#str(ri.freq)
#barplot(ri.freq, main = "Frequency of roads missing stid")
#func <- missing.stid$rd.ri*100.0/nrow(missing.stid)
#plot(x = missing.stid$rd.ri, y = func)
```



***
***
***

Get the most frequent strings in comments column

```{r, tidy=TRUE, cache=TRUE}
freqfunc <- function(x, n){
  tail(sort(table(unlist(strsplit(as.character(x), ", ")))), n)
}
freqfunc(be2olak.data$rd.rp.cm, 10)
```


***

```{r, echo=F, eval=F, tidy=TRUE, cache=TRUE}
library(rmarkdown)
render("data-analysis.Rmd")
```
