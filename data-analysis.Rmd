---
title: "Traffic Data Analysis"
output: 
    html_document:
      toc: true
      toc_float: false
      collapsed: false
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
library(knitr)
library(stargazer)
library(MASS)
library(pracma)
library(tidyr)
library(lubridate)
library(XML)
library(methods)
library(pastecs)
```

### Data Processing

```{r, tidy=TRUE, cache=TRUE}
be2olak.data <- read.csv("all-semi-unique.csv")
summary(be2olak.data)
str(be2olak.data)
```

```{r, tidy=TRUE, cache=TRUE}
#duplicated.rows <- be2olak.data[ which(be2olak.data$rd.rp.cmid == 9344516), ]
#duplicated.rows
```

***

#### Removing not needed columns

By analyzing the distribution of some columns, I found that some of the columns only have one value, so these columns are not needed and should be deleted from the data.

***

```{r, tidy=TRUE, cache=TRUE}
col_ct = sapply(be2olak.data, function(x) length(unique(x)))
print(col_ct)
#be2olak.data <- subset(be2olak.data, select=-c(2:14,22,34))
be2olak.data <- be2olak.data[, !names(be2olak.data) %in% names(col_ct[col_ct==1])]
```

Moreover, the 6 columns (rd.stid, rd.hr, rd.mn, rd.rp.fullnm, rd.img, rd.rp.img) are meaningless so I will remove them:

```{r, tidy=TRUE, cache=TRUE}
col_ct = c("rd.stid", "rd.hr", "rd.mn", "rd.rp.fullnm", "rd.img", "rd.rp.img")
print(col_ct)
```

```{r, tidy=TRUE, cache=TRUE}
print(!names(be2olak.data) %in% col_ct)
be2olak.data <- be2olak.data[, !names(be2olak.data) %in% col_ct]
```

```{r, tidy=TRUE, cache=TRUE}
##nrow(be2olak.data) - length(is.na(be2olak.data$rd.img))
#sum(is.na(be2olak.data$rd.img))
#nrow(be2olak.data)
#be2olak.data[!is.na(be2olak.data$rd.img),]
```

```{r, tidy=TRUE, cache=TRUE}
summary(be2olak.data)
str(be2olak.data)
```

```{r, tidy=TRUE, cache=TRUE}
be2olak.data[duplicated(be2olak.data[,2:ncol(be2olak.data)]),]
stargazer(be2olak.data, type = "text", title = "Descriptive statistics")
```

The proportion of NAs:

```{r, tidy=TRUE, cache=TRUE}
length(be2olak.data[is.na(be2olak.data)])/(ncol(be2olak.data)*nrow(be2olak.data)) 
#barplot(sapply(be2olak.data, function(col) sum(is.na(col))))
```

***

#### Removing Duplicated Rows

```{r, tidy=TRUE, cache=TRUE}
nrow(unique(be2olak.data[c("crawl_date", "rd.rp.stid", "rd.rp.cmid")]))
nrow(unique(be2olak.data[c("rd.ri", "rd.rp.stid", "rd.rp.cmid")]))
nrow(unique(be2olak.data[c("rd.rp.stid", "rd.rp.cmid")]))
nrow(unique(be2olak.data[c("rd.rp.cmid")]))
#dups <- duplicated(be2olak.data[,c(1,15,17)])
#print(dups)
#be2olak.data <- be2olak.data[!duplicated(be2olak.data[,which( colnames(be2olak.data)=="rd.rp.cmid" )]),]
```

Remove duplicated comments:
```{r, tidy=TRUE, cache=TRUE}
dim(be2olak.data)
#be2olak.data <- be2olak.data[!duplicated(be2olak.data[,which( colnames(be2olak.data)=="rd.rp.cmid" )]),]
be2olak.data <- be2olak.data[!duplicated(be2olak.data[,which( colnames(be2olak.data) %in% c("rd.rp.cmid", "rd.rp.stid", "rd.ri") )]),]
dim(be2olak.data)
```


```{r, tidy=TRUE, cache=TRUE}
strq.cmrq <- be2olak.data[which(be2olak.data$rd.strq == be2olak.data$rd.cmrq),]
nrow(strq.cmrq)
head(strq.cmrq[,c(2:13)])
```

```{r, tidy=TRUE, cache=TRUE}
unique.cmid <- aggregate(rd.rp.stid ~ rd.rp.cmid, be2olak.data, length)
duplicated.cmid <- unique.cmid[ which(unique.cmid$rd.rp.stid > 1), ]
head(duplicated.cmid)
tail(duplicated.cmid)
```


```{r, tidy=TRUE, cache=TRUE}
duplicated.cmid.vector <- rbind(duplicated.cmid$rd.rp.cmid)
#print(duplicated.cmid.vector)
duplicated.rows <- be2olak.data[ which(be2olak.data$rd.rp.cmid %in% duplicated.cmid.vector), ]
nrow(duplicated.rows[!duplicated.rows$rd.rp.nm %in% c("bey2ollakgps"),])
```

From above we reached that duplicated rows are only existing when rd.rp.nm is "be2ollakgps"

```{r, tidy=TRUE, cache=TRUE}
stid10.percentage <- nrow(duplicated.rows[which(duplicated.rows$rd.rp.stid == 10),]) * 100.0/ nrow(duplicated.rows)
print(stid10.percentage)
```

Check data summary again after removing the duplicated rows:
```{r, tidy=TRUE, cache=TRUE}
stargazer(be2olak.data, type = "text", title = "Descriptive statistics")
summary(be2olak.data)
str(be2olak.data)
```


The proportion of NAs:

```{r, tidy=TRUE, cache=TRUE}
length(be2olak.data[is.na(be2olak.data)])/(ncol(be2olak.data)*nrow(be2olak.data)) 
```

```{r, tidy=TRUE, cache=TRUE}
col_ct = sapply(be2olak.data, function(x) length(unique(x)))
print(col_ct)
be2olak.data <- be2olak.data[, !names(be2olak.data) %in% names(col_ct[col_ct==1])]
```

Get the most frequent strings in comments column

```{r, tidy=TRUE, cache=TRUE}
freqfunc <- function(x, n){
  tail(sort(table(unlist(strsplit(as.character(x), ", ")))), n)
}
freqfunc(be2olak.data$rd.rp.cm, 10)
```

***

#### Plotting graphs for the different columns

```{r, tidy=TRUE, cache=TRUE}
status <- be2olak.data$rd.rp.stid
status.freq <- table(status)
#pie(status.freq, radius = 1)
print(status.freq)
str(status.freq)
barplot(status.freq)
```

```{r, tidy=TRUE, cache=TRUE}
hist(status,xlab = "Comment status",10)
```

```{r, tidy=TRUE, cache=TRUE}
stid.nas <- sum(is.na(be2olak.data$rd.rp.stid))
print(stid.nas)
stid.nas/(nrow(be2olak.data)) 
ggplot(be2olak.data) + geom_bar(aes(x=rd.rp.stid), fill="gray")
```

By Montoring Be2ollak website, I was able to find most of those status look like:

* 1 --> :D
* 2 --> :)
* 3 --> :|
* 4 --> :(
* 5 --> :'(
* 6 --> ?
* 7 --> Danger Skull
* 10 --> lamb

However, 8 and 9 were difficult to find in the feed.


```{r, tidy=TRUE, cache=TRUE}
#write.csv(be2olak.data, file = "MyData.csv",row.names=FALSE)
```

***

### Feature Engineering

The crawling date is presented in a way that we could make a use of to get the other columns that might help in viewing relations between columns.

Moreover, it will allow us to get the actual date of report from a user, which in case of "za7ma" reports for example, it will get the actual time of the phenomenon.

So to do this, there are 2 main steps:

* First, is to split the crawl date into week day, month, month day, crawl-date (as hours - minutes - seconds) only.

* Then form a new column representing this a formated crawling date.

* Then get the actual reports date (taking into consideration the difference between EET and UCT).

#### Format Crawl Date in a better way

```{r, tidy=TRUE, cache=TRUE}
new.data <- be2olak.data %>% separate(crawl_date, c("Week_day", "Month", "Month_day", "Crawl_date"), extra = "drop", sep = "[ ]+", convert = TRUE, remove = TRUE) %>% unite(Crawl.time, Week_day, Month, Month_day, Crawl_date, sep = " ", remove = FALSE)
summary(new.data)
str(new.data)
new.data$formated.date <- as.POSIXct(strptime(new.data$Crawl.time, format="%a %b %d %H:%M:%S"))
str(new.data)
summary(new.data)
#new.data$Crawl_date <- as.Date(as.character(new.data$Crawl_date))
#str(new.data)
```

***

#### Get Comment's Actual Time

```{r, tidy=TRUE, cache=TRUE}

# Adding 2 hours to get Egypt's local time instead of UTC
# Remove the duration of the report has been posted presented by rd.rp.hr and rd.rp.mn

new.data$comment.time <- new.data$formated.date + hours(2) - hours(new.data$rd.rp.hr) - minutes(new.data$rd.rp.mn)

str(new.data)
summary(new.data)
glimpse(new.data)
#View(new.data)
```

***

Another thing that could be done related to augmenting the data, is to add a column representing the city of each the road being reported on, whether it is "cairo" or "alex". And this is done in the next section.

#### Add the city for each entry

First, get the ids of the roads in Cairo.

```{r, tidy=TRUE, cache=TRUE}

# Get Cairo Road ids

doc <- htmlParse("http://www.bey2ollak.com/Bey2ollak/Traffic?action=getTraffic&ver=1.0&w=320&h=240&deviceType=10&lang=1&protocol=1&city=0&lang=1")
cairo.roads.id <- sapply(getNodeSet(doc, "//ri"), function(x) as.integer(xmlValue(x)))
length(cairo.roads.id)

```

Then, get the ids of the roads in Alex.

```{r, tidy=TRUE, cache=TRUE}

# Get Alex Road ids

doc <- htmlParse("http://www.bey2ollak.com/Bey2ollak/Traffic?action=getTraffic&ver=1.0&w=320&h=240&deviceType=10&lang=1&protocol=1&city=1&lang=1")
alex.roads.id <- sapply(getNodeSet(doc, "//ri"), function(x) as.integer(xmlValue(x)))
length(alex.roads.id)
```

Then, add column representing the city in each row in the dataframe.

```{r, tidy=TRUE, cache=TRUE}
be2olak.data$city <- as.factor(ifelse(be2olak.data$rd.ri %in% cairo.roads.id, "cairo", "alex"))
str(be2olak.data)
levels(be2olak.data$city)
ggplot(be2olak.data) + geom_bar(aes(x = city), fill = "gray")
```

```{r, tidy=TRUE, cache=TRUE}
#stat.desc(be2olak.data)
#tapply(be2olak.data$rd.rp.stid, be2olak.data$city, mean)
```

***

### Exploring the NAs of the rd.rp.stid column

```{r, tidy=TRUE, cache=TRUE}
missing.stid <- be2olak.data[is.na(be2olak.data$rd.rp.stid), c("rd.rp.cm", "rd.ri", "city", "rd.nm")]
str(missing.stid)
summary(missing.stid)
ggplot(missing.stid) + geom_bar(aes(x = city), fill = "gray")
plot(missing.stid$city, main = "city missing ratio")
ggplot(missing.stid) + geom_bar(aes(x = rd.ri), fill = "red")
hist(missing.stid$rd.ri, xlab = "Comment status", 10)
hist(missing.stid$rd.ri, freq = F, xlab = "Comment status", 40)
freqfunc(missing.stid$rd.rp.cm, 80)
```


```{r, tidy=TRUE, cache=TRUE}
missing.stid <- missing.stid %>% separate(rd.nm, c("MainRoad", "PartOfRoad"), extra = "drop", sep = ";", convert = TRUE, remove = F)
missing.stid$MainRoad <- as.factor(missing.stid$MainRoad)
missing.stid$rd.ri <- as.factor(missing.stid$rd.ri)
summary(missing.stid)
#print.data.frame(missing.stid)
ggplot(missing.stid, aes(x=MainRoad, y=rd.ri)) + geom_tile(aes(fill=city))
```

So after analyzing the rows that has stid with NAs so mainly they are highway roads and roads connecting between cities, so it is inconvenient for say that they do belong to one city according to the direction of the road.

Moreover, The comments are mainly about asking about radar and reporting radars or clear statuses.

Those type of roads would need another analysis, but for my analysis I would remove those roads from the original data frame.

```{r, tidy=TRUE, cache=TRUE}
ri.freq <- table(missing.stid$rd.ri)
print(ri.freq)
str(ri.freq)
barplot(ri.freq, main = "Frequency of roads missing stid")
func <- missing.stid$rd.ri*100.0/nrow(missing.stid)
plot(x = missing.stid$rd.ri, y = func)
```


```{r, tidy=TRUE, cache=TRUE}
#write.csv(missing.stid, file = "MissingDataStid.csv",row.names=FALSE)
```


```{r, tidy=TRUE, cache=TRUE}

```

***
### Hypotheses

Here are some ideas for Hypotheses that could be tried

* [1] The ratio of negative reports in Cairo is much higher than in Alex.
* [2] The ratio of negative reports on Firday is less than any other day in the week.
* [3] In (3otl, 7adsa, khatar), the probabitity of uploading an image in a report is significantly higher than in normal times.
* [4] Leaving the default message of a status is higher than of write customized ones.
* [5] [7:30 am - 9 am] and [2 pm - 4 pm] have higher rate of negative reports than in other times of the day.

***

```{r, echo=F, eval=F, tidy=TRUE, cache=TRUE}
library(rmarkdown)
render("data-analysis.Rmd")
```
